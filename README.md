# mHumanEval
A multilingual benchmark to evaluate LLMs on Code Generation. It contains coding prompts in 204 natural langauges.

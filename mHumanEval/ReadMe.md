Here, we have the original mHumanEval benchmark.
Containing 33,456 prompts in total.
Canonical solutions are provided in Python.
For the language codes, we used Flores-200 codes.
